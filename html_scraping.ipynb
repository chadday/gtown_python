{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping with BeautifulSoup\n",
    "\n",
    "Last week, we used the Inspector tool in our browser to take a look at patterns in the HTML of some government websites. This week, we'll use Python and a package called BeautifulSoup to parse that HTML into structured data that we can use. We'll be using an example from one of your homework submissions today.\n",
    "\n",
    "The notebook below is a skeleton of a generic scraper that we'll adapt to the structure of the website selected by the class. This a pretty typical workflow for a scraper project because you'll often use older scrapers you've written as examples for how to scrape new websites. \n",
    "\n",
    "In case you haven't already, let's install BeautifulSoup4 and lxml using pip3.\n",
    "\n",
    "```\n",
    "pip3 install beautifulsoup4\n",
    "\n",
    "pip3 install lxml\n",
    "```\n",
    "\n",
    "Then we'll import the three open source packages we'll be working with today: `BeautifulSoup` (aka `bs4`), `requests` and `pandas`. We'll also be using Python's built in `time` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical scraping: Set our header\n",
    "\n",
    "To scrape a website, we'll be using `requests` to send an https request and return back the response containing the html we want to parse. This is the same thing as when you type a url into your browser and push enter. To make sure we're being ethical and up front about what we're doing, it's good practice to sent a note in the header so that the site admins can see what we're doing if there's any issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_string = ('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36' \n",
    "                 '(KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36 ' \n",
    "                 'Hey there, Chad Day here at The Wall Street Journal. '\n",
    "                 'I am scraping some public data from your site. '\n",
    "                 'You can reach me at chad.day@wsj.com.')\n",
    "\n",
    "header = {'User-Agent': header_string}\n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the url\n",
    "\n",
    "Copy and in the url into a string and assign it a variable, like we do with `url` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://extapps2.oge.gov/FOIAStatus/FOIAResponse.nsf/2FC940AD2A3190BD8525811B004560CE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send the request\n",
    "\n",
    "Now, we'll put them together and return back the html we want to parse. This is often called \"making the soup.\" Below, requests returns the response data from the site. The page html is stored in the `.text` attribute of the response. We pass that text to BeautifulSoup and tell it to parse it using `lxml` an API that turns the text into heirarchical structured data that we can navigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, headers=header)\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the html on the page and try out some searches\n",
    "\n",
    "BeautifulSoup uses the tags, classes, ids and text of the html to locate the pieces you want. The most common methods are `.find()` and `.find_all()`. They do what they sound like: find pieces of the html that match your search criteria. `.find()` locates the first object that matches the criteria, while `.find_all()` locates all of them and returns a list of what it finds.\n",
    "\n",
    "Let's see it in this example using an html table of documents released by the Office of Government Ethics under FOIA. You can find the site [here](https://extapps2.oge.gov/FOIAStatus/FOIAResponse.nsf/2FC940AD2A3190BD8525811B004560CE).\n",
    "\n",
    "This site has a very basic html table in it that we want to extract. The pattern looks like below. First, we see a table tag followed by a tag signifying the start of the body of the table. Then we see a series of `<tr>` tags, which signify the rows of the table. The first row has `<th>` tags for the headers, and the subsequent rows have `<td>` tags containing cells of data.\n",
    "\n",
    "```\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>Tracking Number and Date of Release</th>\n",
    "      <th>Description of Records Sought</th>\n",
    "      <th>Attachment</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>FY 18 - 002 (07/19/2018)</td>\n",
    "      <td>Description ... </td>\n",
    "      <td>\n",
    "        <a href=\"url...\">Link text ... </a>\n",
    "      </td>\n",
    "    </tr>\n",
    "    ...\n",
    "</table>\n",
    "\n",
    "```\n",
    "\n",
    "We'll leverage these patterns, or similar ones in our class example, to extract the data using a for loop and the Python list data structure. A reminder, for loops allow us to do something to each item in a list sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the section of the page containing our data\n",
    "\n",
    "With our example, it's the `<table>` tag but which one. There are multiple tables on the page. Let's use `find_all` to create.a list and then select the table we want using the index of our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = soup.find_all('table')\n",
    "\n",
    "print(f'There are {len(tables)} tables on this page.')\n",
    "\n",
    "str(tables[1])[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = tables[1].find_all('tr')\n",
    "\n",
    "rows[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the data \n",
    "\n",
    "We'll skip the first row as an example because it only contains our headers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rows[1].find_all('td')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a record \n",
    "\n",
    "Below we'll use a Python dictionary to define our record. Remember, it's a key-pair data structure. We'll do this because it's very easy to convert lists of dictionaries into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = {\n",
    "    'number_and_date': data[0].text,\n",
    "    'description': data[1].text,\n",
    "    'url': data[2].find('a').get('href'),\n",
    "}\n",
    "\n",
    "rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the loop\n",
    "\n",
    "Now, let's put it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for row in rows[1:]:\n",
    "    data = row.find_all('td')\n",
    "    url_start = 'https://extapps2.oge.gov/'\n",
    "    url_end = data[2].find('a').get('href')\n",
    "    url = url_start + url_end\n",
    "    rec = {\n",
    "        'number_and_date': data[0].text,\n",
    "        'description': data[1].text,\n",
    "        'url': url,\n",
    "    }\n",
    "    records.append(rec)\n",
    "    \n",
    "records[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a `pandas` dataframe\n",
    "\n",
    "We can pass our records list of dictionaries directly to pandas to create a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(records)\n",
    "\n",
    "print(f'There are {len(df.index)} rows in the dataframe.')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output to a csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/oge_foias.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
